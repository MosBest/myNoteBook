[toc]

# 第一章：绪论
## 归纳偏好
![enter description here][1]
如上图所示，在只知道这6个训练集样本的情况下，通过某一个相同的算法，我们可以得到两个模型（A，B，注：A,B是由一个相同的算法得到的。即A，B在同一个算法的假设空间里）来完美的拟合这6个点。但是对于一个具体的算法，他必须有一个确定的模型，要不然当我们使用这一个算法时，给他一个测试集，时而告诉我们结果为“正”，时而告诉我们结果为“负”。那么怎样从A,B中选择更好的一个模型呢？
这时，算法本身的“偏好”就起到关键的作用。

注：该算法的假设空间为H，则A,B都是H里面的。即A,B是同一个算法的两个不同的假设，只是在这个数据集上面，表现是相同的。

比如，我们的这个算法在某一个数据集上面，虽然得到了两个一样好的假设模型（A,B），但是这个算法更喜欢（偏向）平滑的曲线，因此当我们使用这个算法训练模型时，该算法最终就会选择模型A，而不是B.

任何一个有效的机器学习算法必有其归纳偏好。

归纳偏好可看作学习算法自身在一个可能很庞大的假设空间中对假设进行选择的启发式或“价值观”.（好好体会这句话，说得很精确。我们用某一个算法，训练出一个模型。其实就是在这个算法的假设空间 H 里面找到那个最好的模型h,使这个h，尽可能的逼近真正的f。而所谓的“价值观”，就是这个算法在模型表现都一样的情况下，更“喜欢”哪种情况的模型）

关于分类的算法那么多，为什么没有说那么算法是最好的呢？？
因为不同的算法有不同的归纳偏好，适合处理某一固定的问题。

因此，在具体的现实问题中，要考虑算法的归纳偏好是否与问题本身匹配，大多数时候会直接决定了算法能否取得好的性能.


### 奥卡姆剃刀原理
那么，有没有一般性的原则来引导算法确立“正确的”偏好呢?
“奥卡姆剃刀”(Occam's razor)是一种常用的最基本的原则。

即“若有多个假设与观察一致，则选最简单的那个”.
也就是，如果c，d模型在数据中的表现形式一样，那么就选择简单的那个模型。
c,d可以是同一个算法得到的两个模型，也可以是两个不同的算法得到的模型。

但这个原则并不一定完全可行。比如，什么样的模型最简单？这个问题就就不简单。



## 没有免费的午餐原理 no free lunch theorem
上面我们说的都是同一个算法内不同假设的问题，下面就看单个的算法，而不是他里面的某一个假设。

注：上面的图与下面的图有区别。
上面的图形中，曲线A和曲线B是由同一个算法得到的不同模型。
而下面的图形中，曲线A是算法a基于自身的归纳偏好得到的模型，而曲线B是算法b基于另一个归纳偏好得到的模型。

因此：从这里开始，就不再讨论归纳偏好了。而是讨论不同算法在不同问题下的情况。

![enter description here][2]

从图形可以看出，在某一个的问题下（不同的问题，就是不同的数据集。问题不同，当然数据集就不同），A由于B，而在另一个问题下，B优于A。

换言之，对于一个学习算法a，若它在某些问题上比学习算法b好，则必然存在另一些问题，在那里b比a好.有趣的是，这个结论对任何算法均成立，哪怕是把本书后面将要介绍的一些聪明算法作为a。而将“随机胡猜”这样的笨拙算法作为b.惊讶吗?


所以，NFL定理最重要的寓意，是让我们清楚地认识到，脱离具体问题，空泛地谈论“什么学习算法更好”毫无意义，因为若考虑所有潜在的问题，则所有学习算法都一样好（也就是，对每一个算法，把世界所有的问题都处理一遍，得到的误差平均和，与其他任意一个算法应该都是一样的）.要谈论算法的相对优劣，必须要针对具体的学习问题;在某些问题上表现好的学习算法，在另一些问题上却可能不尽如人意，学习算法自身的归纳偏好与问题是否相配，往往会起到决定性的作用.


  [1]: ./images/xigua_1.png "xigua_1.png"
  [2]: ./images/1474963637955.jpg "1474963637955.jpg"
  [3]: ./images/1474966611463.jpg "1474966611463.jpg"