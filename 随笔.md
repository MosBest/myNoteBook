[toc]
# 我的学习心得，培养自己好的学习习惯，提高自己的学习能力
1. 不会的，想要了解的，第一时间找到　知乎　，维基百科，google
2. 链接不管怎么收藏都不是自己的，一有时间马上把他学习完．一定要看下去，硬着头皮看下去，收获还是挺大的．
3. 看完之后，重要的知识做一下记录
4. 在比赛和项目中，使用机器学习工具（定一下来一个，集中精力去学习这一个），并且分析之前自己对模型与算法的实现与这些工具的实现之间的差异。每一次比赛和项目都要给自己定一个优化的目标（需求驱动学习），使自己对机器学习及数据挖掘有更深层次的理解。当实践遇到瓶颈时，回过头来再进一步夯实理论（凸优化理论，机器学习模型与算法），会发现自己对工具的使用有诸多错误的地方，甚至能够发现工具有更多可改进的地方，这时才真正打开了数据科学的大门。


# 我的特征工程　经验
## 建议
1. 10折交叉验证，如果里面有几折交叉验证的结果很差，那么多半说明这个模型可能出现了过拟合现象。
1. 首先您说的分类正确率是指哪一个指标？Accuracy、Precision、Recall、F1或其他？是交叉验证得来的，还是在测试集上预测得来的？
要让“分类正确率”有10个点以上的提升，我能想到以下方法：
1 特征方案：跟具体业务场景相关，非常重要，非常重要，非常重要！
2 预处理：首先，确定数据是否无量纲化了，如果是使用的线性核的支持向量机，还得确保样本归一化。
3 如果是只考虑正类的分类好坏（Precision、Recall、F1等），还需要考虑类别是否平衡，如果不平衡，一个最直接的方式就是对正类进行过采样，或者对负类进行子采样。
4 泛化：不要过拟合了，可使用交叉验证等手段。
5 调参：如果是使用的RF，其参数n_estimators的默认值为10，一般来说都太小，通过增大该参数，可以获得较不错的分类效果。
总的来说，只要不犯“低级错误”，最有效能提高“分类正确率”的还是特征方案这一途径。
## 参考链接
[听说是大作　特征工程　英文版　必看](http://machinelearningmastery.com/discover-feature-engineering-how-to-engineer-features-and-how-to-get-good-at-it/)
[谁动了我的特征－－特征转换](http://www.cnblogs.com/jasonfreak/p/5619260.html)
[特征工程](http://www.cnblogs.com/jasonfreak/p/5448385.html)
[机器学习中，有哪些特征选择的工程方法](https://www.zhihu.com/question/28641663)
[特征工程](https://www.processon.com/view/link/5577f378e4b0d6a77d9c400c)
[机器学习中的数据清洗与特征处理综述](http://tech.meituan.com/machinelearning-data-feature-process.html)
[常见的特征选择](http://dataunion.org/14072.html)
[特征工程到底是什么？](https://www.zhihu.com/question/29316149)
[机器学习中，有哪些特征选择的工程方法？](https://www.zhihu.com/question/28641663) 
－－－－－－－－－－－－－－－－－－－－－－－－－－－
看过的
[使用哑编码的方式将定性特征转换为定量特征：](http://www.ats.ucla.edu/stat/mult_pkg/faq/general/dummy.htm)


# 模型之间的比较
## RF和GBDT在业务使用场景上 有什么非常明显的不同啊
GBDT的效果一般要好于RF，以前GBDT不容易并行化，现在这也不是问题了，所以用GBDT的越来越多
[Quora上的回答：](https://www.quora.com/When-would-one-use-Random-Forests-o9ver-Gradient-Boosted-Machines-GBMs)
那个xgboost 好像也用的比较多 ,各方面都很优秀,xgboost是对gradient boosting machine的c++实现，在算法上加了一定改进,所以不仅跑的快，而且效果好.
RF用的是bagging思想，并没有特别处理那些分错的数据，所以他的偏差比较大
GBDT用的boosting思想，每一棵树都是对上一棵树分错的数据处理，所以要比RF强些
就是说 GBDT的纠错力更强.
RF有两个好处：1.RF比GBDT更容易调参，2.RF比GBDT更不容易过拟合
但是通过一定的特征工程，RF的这两个优点也渐渐被GBDT取代了，所以一般来说还是用GBDT的多


# 机器学习类
csdn上面有一篇草稿也是随笔

1. 关于 sklearn 中的 Pipeline 机制 
Pipeline管道机制 可以把 多个 函数 集合在一起
```python
pipe_lr = Pipeline([('sc', StandardScaler()),
                    ('pca', PCA(n_components=2)),
                    ('clf', LogisticRegression(random_state=1))
                    ])
```
随后，只要对数据运行pipe_lr的某个方法，就会依次的将上面包含的方法运行。具体请参考[资料详解](http://blog.csdn.net/lanchunhui/article/details/50521648) 
Pipeline实例可以用于fit()和predict（），就像我们有一个正常的分类器

2. 机器学习 sklearn 的==特征选择库== sklearn.feature_selection
可以帮助我们自动的寻找特征
```python
from sklearn.feature_selection import SelectKBest
```
3. [特征工程详解，该博客还对sklearn有很好的解释](http://www.cnblogs.com/jasonfreak/p/5448385.html)
4. [知乎上面关于特征工程的回答](https://www.zhihu.com/question/29316149)
4. 突然发现，sklearn的preprocessing是专们用于对数据进行预处理的。
以下都是上面3中链接的内容，详情还是看3的链接
```python
无量纲化  
  * 标准化 from sklearn.preprocessing import StandardScaler
  * 归一化 from sklearn.preprocessing import Normalizer
  * 区间缩放法  from sklearn.preprocessing import MinMaxScaler
特征二值化  from sklearn.preprocessing import Binarizer
对定性特征哑编码
缺失值
数据变换

```
5. 比较模型，一般是比较精确-召回曲线下面的面积，或者准确率。

